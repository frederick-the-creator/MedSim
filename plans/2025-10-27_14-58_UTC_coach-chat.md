# Implement Gemini Chat with Raw Streaming

### Backend (Node + Express)

- Add POST `POST /api/chat` in `server/routes.ts` that:
  - Validates JSON `{ messages: {role, content}[], transcript: string, assessment: string }` (use zod inline; map client `assistant`â†’Gemini `model`).
  - Build `systemInstruction` in server using a prompt builder that reads a persona text file:
    - Create `shared/prompts/coach_persona.txt` containing ONLY the coach persona text.
    - Add `server/promptBuilder.ts` exporting `buildCoachSystemInstruction(assessment: string, transcript: string): Promise<string>` which loads and caches `coach_persona.txt`, then returns persona + sections: `=== ASSESSMENT ===` and `=== TRANSCRIPT ===`.
    - Optionally add TS path alias `"@server/*": ["./server/*"]` for importing `promptBuilder`.
  - Call Gemini `gemini-2.0-flash` with `generateContentStream` and stream chunks as `text/plain`:
    - Set headers: `Content-Type: text/plain; charset=utf-8`, `Transfer-Encoding: chunked`, `Cache-Control: no-cache`, `X-Accel-Buffering: no`.
    - For each chunk: `res.write(chunk.text())`; on finish: `res.end()`; on error: `res.end()`.

- Example server builder file:
```typescript
// server/promptBuilder.ts
import fs from 'fs/promises';
import path from 'path';
let cachedPersona: string | null = null;

export async function buildCoachSystemInstruction(assessment: string, transcript: string): Promise<string> {
  if (!cachedPersona) {
    const personaPath = path.resolve(process.cwd(), 'shared/prompts/coach_persona.txt');
    cachedPersona = await fs.readFile(personaPath, 'utf8');
  }
  return [
    cachedPersona.trim(),
    '=== ASSESSMENT ===',
    assessment || '(none provided)',
    '=== TRANSCRIPT ===',
    transcript || '(none provided)'
  ].join('\n\n');
}
```

- Essential server streaming loop:
```startLine:endLine:server/routes.ts
// inside /api/chat handler
type SimpleMsg = { role: 'user' | 'assistant'; content: string };
const contents = body.messages.map((m: SimpleMsg) => ({
  role: m.role === 'assistant' ? 'model' : 'user',
  parts: [{ text: m.content }],
}));
const systemInstruction = await buildCoachSystemInstruction(body.assessment, body.transcript);
const model = genAI.getGenerativeModel({ model: 'gemini-2.0-flash', systemInstruction });
const result = await model.generateContentStream({ contents });
res.setHeader('Content-Type', 'text/plain; charset=utf-8');
res.setHeader('Transfer-Encoding', 'chunked');
res.setHeader('Cache-Control', 'no-cache');
res.setHeader('X-Accel-Buffering', 'no');
for await (const chunk of result.stream) {
  res.write(chunk.text());
}
res.end();
```


### Frontend (Vite + React)

- In `client/src/pages/case-detail.tsx`:
  - Keep `assessment` and `transcript` state as-is.
  - Implement `sendChatMessage` that appends the user message, posts to `/api/chat` with `{ messages, transcript, assessment }`, then streams the response and updates the assistant message incrementally.
  - Pass `onSendMessage={sendChatMessage}` and `isLoading` to `ChatInterface`.

- Client streaming snippet:
```startLine:endLine:client/src/pages/case-detail.tsx
const sendChatMessage = async (text: string) => {
  const userMsg = { id: `${Date.now()}`, role: 'user', content: text, timestamp: new Date() };
  setChatMessages(prev => [...prev, userMsg]);
  const assistantId = `${Date.now()}-assist`;
  setChatMessages(prev => [...prev, { id: assistantId, role: 'assistant', content: '', timestamp: new Date() }]);
  setIsAssessmentLoading(true);
  const body = {
    messages: [...chatMessages, userMsg].map(m => ({ role: m.role, content: m.content })),
    transcript: transcript ?? '',
    assessment: assessment ?? '',
  };
  const resp = await fetch('/api/chat', { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify(body) });
  const reader = resp.body?.getReader();
  const decoder = new TextDecoder();
  if (!reader) return;
  let acc = '';
  while (true) {
    const { value, done } = await reader.read();
    if (done) break;
    acc += decoder.decode(value, { stream: true });
    setChatMessages(prev => prev.map(m => m.id === assistantId ? { ...m, content: acc } : m));
  }
  setIsAssessmentLoading(false);
};
```

- Markdown rendering in `client/src/components/ChatInterface.tsx`:
  - Use `react-markdown` + `remark-gfm` to render assistant content.

### Validation and Security

- Require `GEMINI_API_KEY` server-side; fail fast if missing.
- Zod-validate chat payload; cap messages count and content length to prevent abuse.